import requests
from bs4 import BeautifulSoup
import os
import time
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

TELEGRAM_TOKEN = os.environ["TELEGRAM_TOKEN"]
CHAT_ID = os.environ["TELEGRAM_CHAT_ID"]
ADMIN_CHAT_ID = os.environ["ADMIN_CHAT_ID"]

# â”€â”€ Customise these â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SEARCH_LOCATIONS = [
    "London",
    "Surrey",
    "Sheffield",
]

SEARCH_KEYWORDS = [
    "assistant psychologist",
    "research assistant"
]

# A job matches if its title contains ANY of these strings (case-insensitive)
TARGET_TITLES = [
    "assistant psychologist",
    "research assistant",
]

# A job also matches if its employer field contains ANY of these (case-insensitive)
TARGET_EMPLOYERS = [
    "south west london and st georges mental",
    "sw17 0yf",
]

alerted_links = set()  # tracks links we've already sent an alert for

def clean_link(link):
    """Strip query string from a job link so _cb and other params don't cause duplicates."""
    return link.split("?")[0]

# â”€â”€ Base URL builder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_urls():
    """Return one search URL per keyword/location combination."""
    urls = []
    for keyword in SEARCH_KEYWORDS:
        for location in SEARCH_LOCATIONS:
            url = (
                "https://beta.jobs.nhs.uk/candidate/search/results"
                f"?keyword={keyword.replace(' ', '+')}"
                "&skipPhraseSuggester=true"
                "&searchFormType=sortBy"
                "&sort=publicationDateDesc"
                "&language=en"
                f"&location={location.replace(' ', '+')}"
            )
            urls.append((keyword, location, url))
    return urls

# â”€â”€ Matching â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def is_match(job):
    title_lower = job["title"].lower()
    employer_lower = job["employer"].lower()
    title_match = any(t in title_lower for t in TARGET_TITLES)
    employer_match = any(t in employer_lower for t in TARGET_EMPLOYERS)
    return title_match or employer_match

# â”€â”€ Scraping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_page(base_url, page):
    url = f"{base_url}&page={page}&_cb={int(time.time())}"
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; job-alert-bot/1.0)",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
    }
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    return r.text


def parse_jobs(html):
    soup = BeautifulSoup(html, "html.parser")
    jobs = []

    for li in soup.select("li[data-test='search-result']"):
        title_el = li.select_one("a[data-test='search-result-job-title']")
        if not title_el:
            continue
        title = title_el.get_text(strip=True)
        link = title_el.get("href", "")
        if not link.startswith("http"):
            link = "https://beta.jobs.nhs.uk" + link

        employer_el = li.select_one("div[data-test='search-result-location'] h3")
        employer = employer_el.get_text(separator=" ", strip=True) if employer_el else "Unknown"

        date_el = li.select_one("li[data-test='search-result-publicationDate'] strong")
        date_posted = date_el.get_text(strip=True) if date_el else "Unknown"

        closing_el = li.select_one("li[data-test='search-result-closingDate'] strong")
        closing = closing_el.get_text(strip=True) if closing_el else "Unknown"

        salary_el = li.select_one("li[data-test='search-result-salary'] strong")
        salary = salary_el.get_text(strip=True) if salary_el else "Unknown"

        jobs.append({
            "title": title,
            "employer": employer,
            "date_posted": date_posted,
            "closing": closing,
            "salary": salary,
            "link": link,
        })

    return jobs


def get_todays_jobs_for_location(base_url, location, today_str):
    todays_jobs = []
    page = 1

    while True:
        print(f"  [{location}] Fetching page {page}...")
        html = fetch_page(base_url, page)
        jobs = parse_jobs(html)

        if not jobs:
            print(f"  [{location}] No jobs on page {page}, stopping.")
            break

        for i, job in enumerate(jobs, 1):
            print(f"    {i}. {job['title']} | {job['employer']} | Posted: {job['date_posted']}")

        hit_old = False
        for job in jobs:
            if job["date_posted"] == today_str:
                todays_jobs.append({**job, "search_location": location})
            else:
                hit_old = True
                break

        if hit_old:
            print(f"  [{location}] Hit older job, stopping pagination.")
            break

        page += 1
        time.sleep(1)

    return todays_jobs


def get_all_todays_jobs(today_str):
    all_jobs = []
    seen_links = set()

    for keyword, location, url in build_urls():
        jobs = get_todays_jobs_for_location(url, location, today_str)
        for job in jobs:
            if job["link"] not in seen_links:   # deduplicate across locations
                seen_links.add(job["link"])
                all_jobs.append(job)
        time.sleep(2)

    return all_jobs

# â”€â”€ Telegram â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def send_telegram(msg, chat_id):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    resp = requests.post(url, json={"chat_id": chat_id, "text": msg}, timeout=10)
    print(f"  Telegram response ({chat_id}): {resp.status_code} {resp.text}")
    return resp

def alert(msg):
    send_telegram(msg, CHAT_ID)
    send_telegram(msg, ADMIN_CHAT_ID)

def log(msg):
    send_telegram(msg, ADMIN_CHAT_ID)

# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    now = datetime.now()
    current_time_str = now.strftime("%Y-%m-%d %H:%M:%S")
    today_str = now.strftime("%-d %B %Y")

    print(f"\n========================================================")
    print(f"[{current_time_str}] Checking NHS Jobs...")
    print(f"Looking for target date : {today_str}")
    print(f"Locations : {', '.join(SEARCH_LOCATIONS)}")
    print(f"Keywords  : {', '.join(SEARCH_KEYWORDS)}")
    print(f"========================================================\n")

    # NEW: Pass today_str into the scraper
    todays_jobs = get_all_todays_jobs(today_str)

    print(f"\n--- Jobs posted TODAY ({today_str}): {len(todays_jobs)} ---\n")

    matched = [j for j in todays_jobs if is_match(j)]
    print(f"--- Matched jobs to alert: {len(matched)} ---\n")

    new_matched = [j for j in matched if clean_link(j["link"]) not in alerted_links]

    if new_matched:
        alert(f"ğŸ” Found {len(new_matched)} job alert(s) on NHS Jobs today!")
        for job in new_matched:
            print(f"  Alerting: {job['title']} | {job['employer']} ({job['search_location']})")
            msg = (
                f"ğŸš¨ NHS Job Alert!\n\n"
                f"{job['title']}\n"
                f"ğŸ“ Search area: {job['search_location']}\n"
                f"ğŸ¥ {job['employer']}\n"
                f"ğŸ’° {job['salary']}\n"
                f"ğŸ“… Closes: {job['closing']}\n"
                f"ğŸ”— {job['link']}"
            )
            alert(msg)
            alerted_links.add(clean_link(job["link"]))
    else:
        print("No matching jobs today.")
        log("âœ… NHS checker ran - no matching jobs today.")


if __name__ == "__main__":
    while True:
        main()
        print("Sleeping for 15 minutes...")
        time.sleep(900)